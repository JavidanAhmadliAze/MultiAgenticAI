{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ced9c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing import List,  Dict, TypedDict\n",
    "class SupervisorState(MessagesState):\n",
    "    \"\"\"State for the multi-agent research system.\"\"\"\n",
    "    \n",
    "    next_agent : str \n",
    "    task_complete : bool \n",
    "    current_task : str\n",
    "    papers_raw : List[Dict]\n",
    "    ranked_paper : List[Dict]\n",
    "    research_gaps : str \n",
    "    analysis : str \n",
    "    refined_focus: str    \n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d233b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = init_chat_model(\"groq:openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a864740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def scraper_tool(query: str, max_results: int = 3, max_chars: int = 3000) -> Dict:\n",
    "    \"\"\"\n",
    "    Scraper tool: collects recent research papers from arXiv based on the query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query/topic.\n",
    "        max_results (int): Maximum number of papers to retrieve.\n",
    "        max_chars (int): Maximum number of characters from the abstract.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Contains a list of papers with basic metadata.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    papers: List[Dict] = []\n",
    "\n",
    "    for result in client.results(search):  # use Client.results() as recommended\n",
    "        paper_summary = result.summary[:max_chars]\n",
    "        papers.append({\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"abstract\": paper_summary,\n",
    "            \"citations\": 0,  # placeholder, arXiv does not provide citations\n",
    "            \"year\": result.published.year,\n",
    "            \"source\": \"ArXiv\"\n",
    "        })\n",
    "\n",
    "    return {\"papers\": papers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4309bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def create_supervisor_chain():\n",
    "    \"\"\"Creates the supervisor decision chain\"\"\"\n",
    "    \n",
    "    supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a supervisor managing a team of agents:\n",
    "        \n",
    "1. Scraper - Collects research papers from arXiv  \n",
    "2. Ranker - Ranks papers by relevance  \n",
    "3. Gap_Finder - Identifies research gaps  \n",
    "4. Feedback - Refines the research focus  \n",
    "\n",
    "Based on the current state and conversation, decide which agent should work next.\n",
    "If the task is complete, respond with 'DONE'.\n",
    "\n",
    "Current state:\n",
    "- Has raw papers: {has_papers}\n",
    "- Has ranked papers: {has_ranked}\n",
    "- Has research gaps: {has_gaps}\n",
    "- Has refined focus: {has_focus}\n",
    "\n",
    "Respond with ONLY the agent name (scraper/ranker/gap_finder/feedback) or 'DONE'.\"\"\"),\n",
    "        (\"human\", \"{task}\")\n",
    "    ])\n",
    "    \n",
    "    return supervisor_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_ranker_prompt(papers, topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a ranking prompt for the LLM.\n",
    "    \"\"\"\n",
    "    paper_texts = \"\\n\\n\".join(\n",
    "        [f\"{i+1}. Title: {p['title']}\\nAbstract: {p['abstract'][:300]}...\"\n",
    "         for i, p in enumerate(papers)]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert researcher in {topic}.\n",
    "Here are some papers collected on this topic:\n",
    "\n",
    "{paper_texts}\n",
    "\n",
    "Your tasks:\n",
    "1. Rank these papers from most relevant to least relevant for the topic: \"{topic}\".\n",
    "2. For each paper, explain briefly why you placed it at that rank.\n",
    "3. Suggest the top paper that should be read first.\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_gap_finder_prompt(papers, topic: str) -> str:\n",
    "    \"\"\"    Build a prompt for identifying gaps in research.\n",
    "    \"\"\"\n",
    "    paper_texts = \"\\n\\n\".join(\n",
    "        [f\"Title: {p['title']}\\nAbstract: {p['abstract'][:400]}...\"\n",
    "         for p in papers]\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert analyst. Your job is to identify gaps in current research.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Here are the abstracts of recent papers:\n",
    "{paper_texts}\n",
    "\n",
    "Your tasks:\n",
    "1. Summarize the main areas that these papers cover.\n",
    "2. Identify important gaps or underexplored areas in this topic.\n",
    "3. Suggest 2â€“3 specific research questions that remain unanswered.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_feedback_prompt(gaps: str, topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt to interact with the user for refining the search.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an assistant helping refine a research direction.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Based on current analysis, here are the identified gaps:\n",
    "{gaps}\n",
    "\n",
    "Please ask the user clarifying questions to refine the search focus.\n",
    "For example:\n",
    "- Which sub-area is most interesting?\n",
    "- Should we prioritize newer or more cited works?\n",
    "- Do we want theory-focused or application-focused research?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a52c8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "import time\n",
    "from langgraph.graph import END\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import END\n",
    "\n",
    "def supervisor_agent(state: SupervisorState) -> dict:\n",
    "    \"\"\"Supervisor decides the next agent using Groq LLM and current state.\"\"\"\n",
    "\n",
    "    messages = state.get(\"messages\", [])\n",
    "    task = state.get(\"current_task\", messages[-1].content if messages else \"No task\")\n",
    "\n",
    "    # Check what has been done\n",
    "    has_papers = bool(state.get(\"papers_raw\"))\n",
    "    has_ranked = bool(state.get(\"ranked_paper\"))\n",
    "    has_gaps = bool(state.get(\"research_gaps\"))\n",
    "    has_focus = bool(state.get(\"refined_focus\"))\n",
    "\n",
    "    # Invoke LLM chain to get suggested next agent\n",
    "    chain = create_supervisor_chain()\n",
    "    decision = chain.invoke({\n",
    "        \"task\": task,\n",
    "        \"has_papers\": has_papers,\n",
    "        \"has_ranked\": has_ranked,\n",
    "        \"has_gaps\": has_gaps,\n",
    "        \"has_focus\": has_focus\n",
    "    })\n",
    "\n",
    "    # Normalize and interpret decision\n",
    "    decision_text = decision.strip().lower()  # remove whitespace/newlines\n",
    "    print(\"LLM suggested:\", decision_text)\n",
    "\n",
    "    # Determine next agent\n",
    "    if \"done\" in decision_text or has_focus:\n",
    "        next_agent = END\n",
    "        supervisor_msg = \"âœ… Supervisor: All tasks complete! Great work team.\"\n",
    "    elif \"scraper\" in decision_text or not has_papers:\n",
    "        next_agent = \"scraper\"\n",
    "        supervisor_msg = \"ðŸ“‹ Supervisor: Assigning task to Scraper agent...\"\n",
    "    elif \"ranker\" in decision_text or (has_papers and not has_ranked):\n",
    "        next_agent = \"ranker\"\n",
    "        supervisor_msg = \"ðŸ“‹ Supervisor: Papers ready. Assigning task to Ranker agent...\"\n",
    "    elif \"gap_finder\" in decision_text or (has_ranked and not has_gaps):\n",
    "        next_agent = \"gap_finder\"\n",
    "        supervisor_msg = \"ðŸ“‹ Supervisor: Analysis done. Assigning task to Gap Finder agent...\"\n",
    "    elif \"feedback\" in decision_text or (has_gaps and not has_focus):\n",
    "        next_agent = \"feedback\"\n",
    "        supervisor_msg = \"ðŸ“‹ Supervisor: Gaps identified. Assigning task to Feedback agent...\"\n",
    "    else:\n",
    "        next_agent = END\n",
    "        supervisor_msg = \"âœ… Supervisor: Task seems complete.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=supervisor_msg)],\n",
    "        \"next_agent\": next_agent,\n",
    "        \"current_task\": task\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scraper_agent(state: SupervisorState) -> Dict:\n",
    "    \"\"\"Scrapes research papers using the scraper_tool.\"\"\"\n",
    "    if state.get(\"papers_raw\"):  \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"Scraper agent: papers already scraped, skipping.\")],\n",
    "            \"papers_raw\": state[\"papers_raw\"],\n",
    "            \"next_agent\": \"supervisor\"\n",
    "        }\n",
    "\n",
    "    task = state.get(\"current_task\", \"\")\n",
    "    if not task:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"Scraper agent: no current task provided.\")],\n",
    "            \"papers_raw\": [],\n",
    "            \"next_agent\": \"supervisor\"\n",
    "        }\n",
    "\n",
    "    # Directly call scraper_tool (instead of LLM hallucinating a call)\n",
    "    result = scraper_tool.invoke({\"query\": task, \"max_results\": 5})\n",
    "    papers = result.get(\"papers\", [])\n",
    "\n",
    "    agent_message = f\"Scraper agent: retrieved {len(papers)} papers for task '{task}'.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=agent_message)],\n",
    "        \"papers_raw\": papers,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def ranker_agent(state: SupervisorState) -> Dict:\n",
    "    \"\"\"Ranks scraped papers by relevance.\"\"\"\n",
    "    if not state.get(\"papers_raw\"):\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"Ranker agent: no papers available to rank.\")],\n",
    "            \"ranked_paper\": [],\n",
    "            \"analysis\": \"\",\n",
    "            \"next_agent\": \"supervisor\"\n",
    "        }\n",
    "\n",
    "    prompt = build_ranker_prompt(state.get(\"papers_raw\"), state.get(\"current_task\"))\n",
    "    time.sleep(1)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    summary = response.content[:3000]\n",
    "\n",
    "    agent_message = (\n",
    "        f\"Ranker agent: I ranked {len(state.get('papers_raw', []))} papers \"\n",
    "        f\"for task '{state.get('current_task')}'. Summary: {summary}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=agent_message)],\n",
    "        \"ranked_paper\": state.get(\"papers_raw\"),  # placeholder, could be improved with real ranking\n",
    "        \"analysis\": summary,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def gap_finder_agent(state: SupervisorState) -> Dict:\n",
    "    \"\"\"Identifies research gaps in the ranked papers.\"\"\"\n",
    "    if not state.get(\"ranked_paper\"):\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=\"Gap Finder agent: no ranked papers available.\")],\n",
    "            \"research_gaps\": \"\",\n",
    "            \"next_agent\": \"feedback\"\n",
    "        }\n",
    "\n",
    "    prompt = build_gap_finder_prompt(state.get(\"ranked_paper\"), state.get(\"current_task\"))\n",
    "    time.sleep(1)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    summary = response.content[:3000]\n",
    "\n",
    "    agent_message = f\"Gap Finder agent: I identified gaps for task '{state.get('current_task')}'. Summary: {summary}\"\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=agent_message)],\n",
    "        \"research_gaps\": summary,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }\n",
    "\n",
    "\n",
    "def feedback_agent(state: SupervisorState) -> Dict:\n",
    "    \"\"\"Refines research focus.\"\"\"\n",
    "    if not state.get(\"research_gaps\"):\n",
    "        state[\"refined_focus\"] = \"\"\n",
    "        state[\"next_agent\"] = END\n",
    "        return state\n",
    "\n",
    "    prompt = build_feedback_prompt(state.get(\"research_gaps\"), state.get(\"current_task\"))\n",
    "    time.sleep(1)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    summary = response.content[:3000]\n",
    "    \n",
    "    state[\"refined_focus\"] = summary\n",
    "    state[\"next_agent\"] = END\n",
    "    \n",
    "    agent_message = f\" Feedback agent: I completed feedback for {state.get(\"current_task\")}. Here is the summary{summary} \"\n",
    "    \n",
    "    return{\n",
    "        \"messages\" : [AIMessage(content=agent_message)],\n",
    "        \"refined_focus\" : summary,\n",
    "        \"next_agent\" : \"supervisor\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM suggested: scraper\n",
      "Agent: supervisor\n",
      "Message content: ðŸ“‹ Supervisor: Assigning task to Scraper agent...\n",
      "---\n",
      "Agent: scraper\n",
      "Message content: Scraper agent: retrieved 5 papers for task 'Quantum Machine Learning'.\n",
      "---\n",
      "LLM suggested: ranker\n",
      "Agent: supervisor\n",
      "Message content: ðŸ“‹ Supervisor: Papers ready. Assigning task to Ranker agent...\n",
      "---\n",
      "Agent: ranker\n",
      "Message content: Ranker agent: I ranked 5 papers for task 'Quantum Machine Learning'. Summary: Here's a ranking of the papers based on relevance to \"Quantum Machine Learning\", along with explanations:\n",
      "\n",
      "**1. High-capacity associative memory in a quantum-optical spin glass**\n",
      "\n",
      "* **Relevance:** This paper directly explores a quantum mechanical system (spin glass) and applies it to a classic machine learning problem (associative memory). It delves into the potential of quantum phenomena for enhancing memory capabilities. This is a core area of research in Quantum Machine Learning.\n",
      "\n",
      "**2.  Dynamic Relational Priming Improves Transformer in Multivariate Time Series**\n",
      "\n",
      "* **Relevance:** While not explicitly quantum, this paper explores novel techniques for improving Transformer models, which are foundational to many modern machine learning architectures. Advancements in classical machine learning often pave the way for quantum adaptations. \n",
      "\n",
      "**3. Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks**\n",
      "\n",
      "* **Relevance:**  This paper uses graph neural networks (GNNs), a type of machine learning model. GNNs have potential applications in quantum machine learning, particularly for graph-structured data. However, the focus here is primarily on astrophysics and not quantum computing.\n",
      "\n",
      "**4. OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling**\n",
      "\n",
      "* **Relevance:** This paper introduces a large-scale dataset for 4D world modeling. While datasets are crucial for training any machine learning model, including quantum ones, this paper doesn't directly address quantum machine learning techniques.\n",
      "\n",
      "**5. Domain-Adaptive Pretraining Improves Primate Behavior Recognition**\n",
      "\n",
      "* **Relevance:** This paper focuses on computer vision and animal behavior recognition, utilizing domain-adaptive pretraining techniques. While important for machine learning in general, it lacks a direct connection to quantum machine learning.\n",
      "\n",
      "\n",
      "**Suggested First Read:**\n",
      "\n",
      "**\"High-capacity associative memory in a quantum-optical spin glass\"**  This paper offers the most direct and insightful exploration of quantum machine learning concepts within the provided selection. \n",
      "\n",
      "---\n",
      "LLM suggested: gap_finder\n",
      "Agent: supervisor\n",
      "Message content: ðŸ“‹ Supervisor: Analysis done. Assigning task to Gap Finder agent...\n",
      "---\n",
      "Agent: gap_finder\n",
      "Message content: Gap Finder agent: I identified gaps for task 'Quantum Machine Learning'. Summary: ## Quantum Machine Learning: Gaps and Unanswered Questions\n",
      "\n",
      "**1. Main Areas Covered:**\n",
      "\n",
      "These papers highlight several key areas within quantum machine learning:\n",
      "\n",
      "* **Quantum Optimization:** \"High-capacity associative memory in a quantum-optical spin glass\" explores using quantum systems to overcome limitations of classical associative memory models.\n",
      "* **Quantum Data Representation and Learning:** \"OmniWorld\" focuses on building a comprehensive dataset for 4D world modeling, crucial for training quantum models capable of handling complex spatiotemporal data.\n",
      "* **Quantum Applications in Scientific Domains:** \"Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks\" demonstrates the potential of quantum-enhanced machine learning for scientific discovery in astrophysics.\n",
      "* **Improving Quantum Model Architectures:**  \"Dynamic Relational Priming Improves Transformer in Multivariate Time Series\" investigates enhancing transformer models with dynamic relational priming, potentially applicable to quantum-based architectures.\n",
      "* **Data-Efficiency in Quantum Machine Learning:** \"Domain-Adaptive Pretraining Improves Primate Behavior Recognition\" highlights the need for data-efficient learning strategies in quantum machine learning, particularly when dealing with limited labeled data.\n",
      "\n",
      "**2. Important Gaps and Underexplored Areas:**\n",
      "\n",
      "Despite these advancements, several crucial gaps remain in quantum machine learning research:\n",
      "\n",
      "* **Scalability:** Quantum computers are still in their nascent stages, and scaling up quantum machine learning algorithms to handle large datasets and complex problems remains a significant challenge.\n",
      "* **Theoretical Understanding:** While practical applications are emerging, a deeper theoretical understanding of how quantum algorithms provide advantages for specific machine learning tasks is still lacking.\n",
      "* **Hardware-Software Co-design:**  There is a need for closer collaboration between hardware and software developers to optimize quantum algorithms for specific quantum hardware platforms.\n",
      "* **Explainability and Interpretability:** Quantum machine learning models often operate in a highly complex and opaque manner, making it difficult to interpret their decisions and understand their inner workings. Improving the explainability of quantum models is crucial for their wider adoption.\n",
      "\n",
      "**3. Research Questions:**\n",
      "\n",
      "Here are some specific research questions that remain unanswered:\n",
      "\n",
      "* **How can we develop efficient quantum algorithms for training deep learning models on large datasets, surpassing the capabilities of classical counterparts?**\n",
      "* **What are the fundamental theoretical limits of quantum machine learning, and how can we leverage quantum properties to achieve computational advantages in specific machine learning tasks?**\n",
      "* **Can we design quantum machine learning models that are inherently more interpretable and explainable than their classical counterparts, enabling better trust and \n",
      "---\n",
      "LLM suggested: feedback\n",
      "Agent: supervisor\n",
      "Message content: ðŸ“‹ Supervisor: Gaps identified. Assigning task to Feedback agent...\n",
      "---\n",
      "Agent: feedback\n",
      "Message content:  Feedback agent: I completed feedback for Quantum Machine Learning. Here is the summaryThis is a great start! To help me refine the research direction, could you tell me:\n",
      "\n",
      "1. **Which of the identified gaps and research questions resonates most strongly with your interests?** This will help us focus on a specific area within quantum machine learning.\n",
      "2. **Are you more interested in exploring the theoretical foundations of quantum machine learning or in investigating practical applications and algorithms?**  Understanding your preference will help determine the type of research we prioritize.\n",
      "3. **Do you have a specific application domain in mind?** For example, are you interested in quantum machine learning for drug discovery, materials science, or finance?  Focusing on a particular application could lead to more targeted and impactful research. \n",
      "\n",
      "\n",
      "Once I have a better understanding of your priorities, I can help you identify relevant research papers and suggest specific research directions.\n",
      " \n",
      "---\n",
      "LLM suggested: done\n",
      "Agent: supervisor\n",
      "Message content: âœ… Supervisor: All tasks complete! Great work team.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def router(state: Dict) -> str:\n",
    "    \"\"\"Router follows supervisor's decision, normalizing output.\"\"\"\n",
    "    if state.get(\"task_complete\"):\n",
    "        return END\n",
    "    return state.get(\"next_agent\", \"\").strip().lower()\n",
    "\n",
    "\n",
    "def build_workflow(llm):\n",
    "    \"\"\"Builds the multi-agent research workflow.\"\"\"\n",
    "    workflow = StateGraph(SupervisorState)\n",
    "\n",
    "    # Add agents as nodes\n",
    "    workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "    workflow.add_node(\"scraper\", scraper_agent)\n",
    "    workflow.add_node(\"ranker\", ranker_agent)\n",
    "    workflow.add_node(\"gap_finder\", gap_finder_agent)\n",
    "    workflow.add_node(\"feedback\", feedback_agent)\n",
    "\n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "    # Add conditional routing from supervisor\n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        router,\n",
    "        {\n",
    "            \"supervisor\": \"supervisor\",\n",
    "            \"scraper\": \"scraper\",\n",
    "            \"ranker\": \"ranker\",\n",
    "            \"gap_finder\": \"gap_finder\",\n",
    "            \"feedback\": \"feedback\",\n",
    "            END: END,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add linear routing for other agents â†’ always back to supervisor\n",
    "    workflow.add_edge(\"scraper\", \"supervisor\")\n",
    "    workflow.add_edge(\"ranker\", \"supervisor\")\n",
    "    workflow.add_edge(\"gap_finder\", \"supervisor\")\n",
    "    workflow.add_edge(\"feedback\", \"supervisor\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm = init_chat_model(\"groq:gemma2-9b-it\")\n",
    "    graph = build_workflow(llm)\n",
    "\n",
    "    # Initial state\n",
    "    state = {\n",
    "        \"next_agent\": \"supervisor\",\n",
    "        \"task_complete\": False,\n",
    "        \"current_task\": \"Quantum Machine Learning\",\n",
    "        \"papers_raw\": [],\n",
    "        \"ranked_paper\": [],\n",
    "        \"research_gaps\": \"\",\n",
    "        \"analysis\": \"\",\n",
    "        \"refined_focus\": \"\",\n",
    "    }\n",
    "\n",
    "    # Run compiled workflow\n",
    "for step in graph.stream(state):\n",
    "    for agent, output in step.items():\n",
    "        print(f\"Agent: {agent}\")\n",
    "        messages = output.get(\"messages\", [])\n",
    "        for msg in messages:\n",
    "            # Here is where .content is used\n",
    "            print(\"Message content:\", msg.content)\n",
    "        print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiAgentAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
